{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVrkoj5FNmsI1L1Ed3+bQP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# INF581 Lab5: Reinforcement Learning - TD Imitation Learning\n","\n","<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2023/master/logo.jpg\" style=\"float: left; width: 15%\" />\n","\n","[INF581-2023](https://moodle.polytechnique.fr/course/view.php?id=14259) Lab session #7\n","\n","2022-2023 Mohamed ALAMI"],"metadata":{"id":"UjgusuFRty18"}},{"cell_type":"code","source":["# These installs are necessary for packages compatibility and visualization\n","!sudo apt install swig\n","!apt-get install -y xvfb x11-utils\n","!pip install pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*\n","!apt-get update && apt-get install ffmpeg freeglut3-dev xvfb\n","!pip install stable-baselines3[extra] pyglet==1.5.27\n","!pip install sb3_contrib\n","!pip install box2d-py\n","!pip install gym[box2d]==0.25\n"],"metadata":{"id":"52DCBLTYt1Ac"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import stable_baselines3\n","print(stable_baselines3.__version__)\n","import gym\n","print(gym.__version__)\n","from gym import spaces\n","import numpy as np\n","import time\n","import pickle as pkl\n","from tqdm import trange, tqdm_notebook\n","import matplotlib.pyplot as plt\n","from IPython import display\n","from matplotlib.pyplot import figure\n","from matplotlib.animation import FuncAnimation\n","from IPython.display import HTML\n","import pyvirtualdisplay\n","from stable_baselines3 import PPO\n","from stable_baselines3.ppo import MlpPolicy\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.utils.data as data"],"metadata":{"id":"c4Qz65gCt4VH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#These functions are necessary to produce videos of your agent at work\n","\n","def update_scene(num, frames, patch):\n","    patch.set_data(frames[num])\n","    return patch,\n","\n","def plot_animation(frames, repeat=False, interval=40):\n","    fig = plt.figure()\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","    anim = FuncAnimation(\n","        fig, update_scene, fargs=(frames, patch),\n","        frames=len(frames), repeat=repeat, interval=interval)\n","    plt.close()\n","    return anim"],"metadata":{"id":"dzE9wenNt9Zv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Behavioral Cloning\n","\n","Behavioral is the simplest approach to imitation learning. The concept is straightforwardy: An expert plays a game perfectly and the learning agent has just to act as a copy cat. For each state, it has to predict what the expert would have done. Usually the expert demonstrations are typically retrieved from recording human behaviour, but for the purpose of the exercise, we well consider fully trained RL models as expert and we will try to recover their optimal policy through behavioral cloning.\n","\n","The algorithm is recalled below. \n","\n","<b>Input</b>:<br>\n","\t$\\quad\\quad$ Environment and expert model<br>\n","<b>Algorithm parameter</b>:<br>\n","\t$\\quad\\quad$ number of episodes $n$; number of epochs: epochs\n","\n","Train the expert model and get $\\pi_{\\text{exp}}$ <br>\n","For episode in range($n$): <br>\n","$\\quad$ Initialise environment at state $s$ <br>\n","$\\quad$ While not done: <br>\n","$\\quad \\quad$ action $a = \\pi_{exp}(s)$ <br>\n","$\\quad \\quad$  $s_{1}, done = env($a$)$ <br>\n","$\\quad \\quad$ store ($s,a$) in dictionary<br>\n","$\\quad \\quad$ $s=s_1$ <br>\n","\n","Initialise model $\\pi$ <br>\n","For $k$ in range(epochs) <br>\n","$\\quad$ Train $\\pi$ for each $(s,a)$ in dictionary such as $\\pi(s)=a$\n","\n"],"metadata":{"id":"CeburdnKuA2Q"}},{"cell_type":"markdown","source":["#### Cartpole\n","\n","We will use The Cartpole environment as a first example. More details on this environment are available [here](https://www.gymlibrary.dev/environments/classic_control/cart_pole/) "],"metadata":{"id":"S96re0EouGJS"}},{"cell_type":"code","source":["env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"],"metadata":{"id":"Dl8U2FYQuIjn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The aim of this lab is not to train the expert so we can either consider models that are already trained on Cartpole or use Stable_baselines, a framework that allows to train several heavily tested famous algorithms in one line. More details [here](https://stable-baselines.readthedocs.io/en/master/).\n","\n","You can either train the imported model (should take 10 min max) or load a trained model available on moodle"],"metadata":{"id":"W5wdh71vuOrH"}},{"cell_type":"code","source":["#we first initialise the model by saying that we will use the PPO algorithm and use a classic\n","# MLP parameterisation\n","cartpole_expert = PPO(MlpPolicy, env, verbose=1)\n","\n","#Then the model can easily be trained in just one line\n","#cartpole_expert.learn(total_timesteps=250000)"],"metadata":{"id":"mypw6uhTuL6_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#In order to save or load the model you need to install drive \n","#For this click on the folder logo on the left and then on \"install drive\"\n","#Copy the path to your current folder here\n","\n","PATH = \"/content/drive/MyDrive/Lab_IRL/\""],"metadata":{"id":"Dl7YseFKuS8X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#You can save or load your model easily\n","\n","#cartpole_expert.save(PATH + \"cartpole.zip\")\n","cartpole_expert = PPO.load(PATH + \"cartpole.zip\")"],"metadata":{"id":"6iICDccvuUv_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The next function will help us generate a video by letting the expert play on the environment for n timesteps"],"metadata":{"id":"-i6dmbb3uX6r"}},{"cell_type":"code","source":["'''\n","Create gifs and render it\n","'''\n","def generate_video(env, model, n_timesteps=300, initial_state=None):\n","  if initial_state is None:\n","    obs = env.reset()\n","  else:\n","    env = env.unwrapped # to access the inner functionalities of the class\n","    env.state = initial_state\n","    obs = env.state\n","\n","  figure(figsize=(8, 6), dpi=80)\n","\n","  # use False con Xvfb \n","  _display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n","  _ = _display.start()\n","  #model = model.float()\n","\n","  frames = []\n","\n","  for t in range(n_timesteps):\n","    #BEGIN ANSWER\n","      #TO DO: Simulate n steps of model playing in the environment\n","    #END ANSWER\n","\n","    frame = env.render(mode='rgb_array')\n","    frames.append(frame)\n","    #env.render()\n","    time.sleep(.025)\n","\n","  anim = plot_animation(frames)\n","  return anim"],"metadata":{"id":"Pu6xQO6-uXB2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make('CartPole-v1', new_step_api=True)\n","anim = generate_video(env, cartpole_expert)\n","HTML(anim.to_html5_video())"],"metadata":{"id":"CZnmRPgduyrG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This function is used to create expert trajectories that will be used in our Behavior Cloning model "],"metadata":{"id":"ghiykcaSu1QA"}},{"cell_type":"code","source":["def generate_expert_traj(model, env, n_episodes=100):\n","  # Sanity check\n","  assert (isinstance(env.observation_space, spaces.Box) or\n","            isinstance(env.observation_space, spaces.Discrete)), \"Observation space type not supported\"\n","\n","  assert (isinstance(env.action_space, spaces.Box) or\n","            isinstance(env.action_space, spaces.Discrete)), \"Action space type not supported\"\n","\n","  actions = []\n","  observations = []\n","  rewards = []\n","  episode_returns = np.zeros((n_episodes,))\n","  episode_starts = []  #Index of new episodes initial states\n","\n","  obs = env.reset()\n","  episode_starts.append(True)\n","  reward_sum = 0.0\n","  idx = 0\n","\n","  for ep_idx in tqdm_notebook(range(n_episodes), desc='Epoch', leave=True):\n","\n","    #BEGIN ANSWER\n","\n","    #TO DO: Populate observations, actions and rewards lists\n","    \n","    #END ANSWER\n","\n","  if isinstance(env.observation_space, spaces.Box):\n","    observations = np.concatenate(observations).reshape((-1,) + env.observation_space.shape)\n","  elif isinstance(env.observation_space, spaces.Discrete):\n","    observations = np.array(observations).reshape((-1, 1))\n","  \n","  if isinstance(env.action_space, spaces.Box):\n","      actions = np.concatenate(actions).reshape((-1,) + env.action_space.shape)\n","  elif isinstance(env.action_space, spaces.Discrete):\n","      actions = np.array(actions).reshape((-1, 1))\n","      \n","  rewards = np.array(rewards)\n","  episode_starts = np.array(episode_starts[:-1])\n","\n","  assert len(observations) == len(actions)\n","\n","  numpy_dict = {\n","        'actions': actions,\n","        'obs': observations,\n","        'rewards': rewards,\n","        'episode_returns': episode_returns,\n","        'episode_starts': episode_starts\n","    }\n","\n","  for key, val in numpy_dict.items():\n","    print(key, val.shape)\n","    \n","  env.close()\n","\n","  return numpy_dict"],"metadata":{"id":"noO8zs5Tu1vs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Generate expert trajectories\n","cartpole_demos = generate_expert_traj(cartpole_expert, env)"],"metadata":{"id":"WfAKgI6CvKnp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Define BC Model as NN\n","\n","Specs:\n","NN: 3 layers, the middle layer should be of size (32,64)\n","Loss: choose it wisely\n","'''\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","    \n","    #BEGIN ANSWER\n","        #TO DO: Define network as described above\n","    #END ANSWER\n","\n","    def forward(self, x):\n","      #BEGIN ANSWER\n","    \t  #TO DO: Define forward funcion and activations\n","      #END ANSWER\n","    \treturn x"],"metadata":{"id":"kmCS_ravvNeR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This function transforms the dataset of expert state-actions pairs into a Dataset class\n","#That makes it easier to train in batches\n","\n","class NumpyDataset(data.Dataset):\n","\n","    def __init__(self, array, transform=None):\n","        super().__init__()\n","        self.obs = torch.FloatTensor(array[\"obs\"])\n","        self.actions = torch.FloatTensor(array[\"actions\"])\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.obs)\n","\n","    def __getitem__(self, index):\n","        obs = self.obs[index]\n","        action = self.actions[index]\n","        if self.transform:\n","            obs = self.transform(obs)\n","            action = self.transform(action)\n","        return obs, action\n"],"metadata":{"id":"qx3-hxD4vdZd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader_args = dict(batch_size=128, shuffle=True)\n","train_dset = NumpyDataset(cartpole_demos)\n","train_loader = data.DataLoader(train_dset, **loader_args)\n","device = \"cpu\""],"metadata":{"id":"5VsIUKWjve-t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_train_curves(epochs, train_losses, test_losses, title=''):\n","    x = np.linspace(0, epochs, len(train_losses))\n","    plt.figure()\n","    plt.plot(x, train_losses, label='train_loss')\n","    #if test_losses:\n","        #plt.plot(x, test_losses, label='test_loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title(title)\n","    plt.legend()\n","    plt.show()\n","  \n","def train(model, train_loader, criterion, optimizer):\n","    model.train()\n","    total_loss = 0\n","\n","    for obs, action in train_loader:\n","\n","\n","      # TO DO: Implement the training loop that is done in an epoch. This function is called in the function train_epochs below\n","\n","\n","\n","      return avg_loss.item()\n","\n","def train_epochs(model, train_loader, criterion, test_loader=None, train_args=None, plot=True):\n","    # training parameters\n","    epochs, lr = train_args['epochs'], train_args['lr']\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","    train_losses, test_losses = [], []\n","    for epoch in tqdm_notebook(range(epochs), desc='Epoch', leave=False):\n","        model.train()\n","        train_loss = train(model, train_loader, criterion, optimizer)\n","        train_losses.append(train_loss)\n","           \n","    if plot:\n","        plot_train_curves(epochs, train_losses, test_losses, title='Training Curve')\n","    return train_losses, test_losses"],"metadata":{"id":"m2HlVgj7vi_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#TO DO: Choose the right criterion\n","#criterion = ...\n","\n","\n","cartpole_BC_model = Net()\n","train_args = dict(epochs=3000, lr=0.0001)\n","train_losses, test_losses = train_epochs(cartpole_BC_model, train_loader, criterion, train_args=train_args)"],"metadata":{"id":"r3RyW_x0v0FJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["anim = generate_video(env, cartpole_BC_model)\n","HTML(anim.to_html5_video())"],"metadata":{"id":"gzjl_idBwA0s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Even with a very quick training, we see that the agent is pretty good. But is it that good ? "],"metadata":{"id":"jDdtYdq2wEQP"}},{"cell_type":"markdown","source":["Now let's try with a different initial state"],"metadata":{"id":"T9QNRfdqwHKa"}},{"cell_type":"code","source":["obs = env.reset()\n","state_init = np.array([1.0, obs[1], obs[2], obs[3]]) #we slightly move the initial position of the cart\n","\n","anim = generate_video(env, cartpole_BC_model, initial_state=state_init)\n","HTML(anim.to_html5_video())"],"metadata":{"id":"e8y8oGJPwE8F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When changing the initial state, we put the agent in a situation the expert has never seen; therefore the agent is lost and does not know what is th good action to perform. \n","\n","Actually it is not even necessary to change the initial state. The agent is never absolutely perfect, so with a long enough horizon, it will make some mistakes that stack up and drive it into states that the expert has never seen (as it only explores best states).  "],"metadata":{"id":"7JNTFHgbwMyI"}},{"cell_type":"markdown","source":["### Lunar Lander"],"metadata":{"id":"Y9-IZQajwPVD"}},{"cell_type":"markdown","source":["Let's test with the Lunar Lander environment"],"metadata":{"id":"TjUgfGfpwS_v"}},{"cell_type":"code","source":["env = gym.make('LunarLander-v2')\n","\n","#model = PPO(MlpPolicy, env, verbose=1)\n","#model.learn(total_timesteps=250000)"],"metadata":{"id":"IBAYUtdMwLpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model.save(PATH + \"lander.zip\")"],"metadata":{"id":"qlP33j1awXGK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["expert_lander = PPO.load(PATH + \"lander.zip\")"],"metadata":{"id":"dZHuK-LVwZJh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make('LunarLander-v2', new_step_api=True)\n","anim = generate_video(env, expert_lander, 400)\n","HTML(anim.to_html5_video())"],"metadata":{"id":"Kmaxq87mwa17"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Lander_demos = generate_expert_traj(expert_lander, env)"],"metadata":{"id":"cl9xhg5gwckS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Define BC Model as NN\n","\n","Specs:\n","NN: 5 layers. Width of hidden layers: 24,64,128,256\n","Loss and activation functions to define\n","'''\n","class Lander_Net(nn.Module):\n","    def __init__(self):\n","        super(Lander_Net, self).__init__()\n","        #BEGIN ANSWER\n","          #TO DO: define NN as described above\n","        #END ANSWER\n","\n","    def forward(self, x):\n","      #BEGIN ANSWER\n","      # TO DO: Define forward function and activations\n","      #END ANSWER\n","      return x"],"metadata":{"id":"FwRX5UrAwfYf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader_args = dict(batch_size=128, shuffle=True)\n","train_dset = NumpyDataset(Lander_demos)\n","train_loader = data.DataLoader(train_dset, **loader_args)\n","\n","#BEGIN ANSWER\n","#criterion = ...\n","#END ANSWER\n","\n","Lander_BC_model = Lander_Net()\n","train_args = dict(epochs=2000, lr=0.001)\n","train_losses, test_losses = train_epochs(Lander_BC_model, train_loader, criterion, train_args=train_args)"],"metadata":{"id":"XbT6bDRbwr1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make('LunarLander-v2', new_step_api=True)\n","anim = generate_video(env, Lander_BC_model, 500)\n","HTML(anim.to_html5_video())"],"metadata":{"id":"hjUxK79xxA8n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Lunar Lander stochastic"],"metadata":{"id":"Z3gQRCQjxCoM"}},{"cell_type":"markdown","source":["Now let's try something else. We will add wind in the environment to make the dynamics of the environement stochastic. "],"metadata":{"id":"NUevuppQxFrG"}},{"cell_type":"code","source":["env = gym.make('LunarLander-v2', enable_wind=True)\n","#model = PPO(MlpPolicy, env, verbose=1)\n","#model.learn(total_timesteps=250000)"],"metadata":{"id":"yZ9dkMwJxDLh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model.save(PATH + \"windy_lander.zip\")"],"metadata":{"id":"hFbzG4PExJwS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["expert_windy_lander = PPO.load(PATH + \"windy_lander.zip\")"],"metadata":{"id":"WSn-nllPxLSs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make('LunarLander-v2', enable_wind=True, new_step_api=True)\n","anim = generate_video(env, expert_windy_lander, 400)\n","HTML(anim.to_html5_video())"],"metadata":{"id":"cI8wteP1xM2l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Lander_demos = generate_expert_traj(expert_windy_lander, env)"],"metadata":{"id":"FEomMcEExOo8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = \"cpu\"\n","loader_args = dict(batch_size=128, shuffle=True)\n","train_dset = NumpyDataset(Lander_demos)\n","train_loader = data.DataLoader(train_dset, **loader_args)\n","\n","#BEGIN ANSWER\n","#criterion = ...\n","#END ANSWER\n","\n","Windy_Lander_BC_model = Lander_Net()\n","train_args = dict(epochs=2000, lr=0.001)\n","train_losses, test_losses = train_epochs(Windy_Lander_BC_model, train_loader, criterion, train_args=train_args)"],"metadata":{"id":"-wxJnKuJxQq_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["anim = generate_video(env, Windy_Lander_BC_model, 500)\n","HTML(anim.to_html5_video())"],"metadata":{"id":"TluH5OLFxUzu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The agent fails because stochastic environments increases \"mistakes\" as well as the chances to end in non optimal states that the expert never encountered"],"metadata":{"id":"_hkzBF8FxWqF"}},{"cell_type":"markdown","source":["### DAgger"],"metadata":{"id":"EZRvmG-vxbcI"}},{"cell_type":"markdown","source":["To mitigate that problem, a classic solution is to use DAgger algorithm (Dataset Aggregation). The idea is simple. We just have to create a feedback loop between the agent and the expert. After each roll or episode of the agent, its trajectory (the list of states) it encountered are given back to the expert, who attribute to them the right action to perform. The nex state-action pairs are then added to the original dataset, and a new training is done on the aggregated dataset"],"metadata":{"id":"IKBw863ZxfCm"}},{"cell_type":"markdown","source":["Below is the algorithm's pseudocode: <br>\n","<b>Input</b>:<br>\n","environment, expert model, agent model, number of iterations, number of trajectories to sample from the agent at each iteration <br>\n","\n","<b>Algorithm</b>: <br>\n","\n","Initialise dataset <br>\n","<b>FOR</b> $i$ in range(number of iterations): <br>\n","$\\quad$ traj $\\xleftarrow{}$generate model trajectories <br>\n","$\\quad$ get states from traj <br>\n","$\\quad$ get expert actions for states <br>\n","$\\quad$ add states and actions to dataset <br>\n","$\\quad$ train agent on dataset"],"metadata":{"id":"oEG31ghBxh8r"}},{"cell_type":"code","source":["#function to extract expert actions from given states\n","def get_expert_actions(model, states):\n","  #BEGIN ANSWER \n","  # actions = ...\n","  #END ANSWER\n","  return actions\n","\n","def DAgger(env, expert, model, n_iterations=10, n_traj=10, iter_plot=1):\n","\n","  dataset = {'obs': None, 'actions': None}\n","  losses = []\n","  #BEGIN ANSWER\n","  #criterion = ...\n","  #END ANSWER\n","\n","  for i in tqdm_notebook(range(n_iterations)):\n","    #BEGIN ANSWER\n","\n","    #TO DO: Dataset aggregation\n","    \n","    #END ANSWERS\n","\n","    train_dset = NumpyDataset(dataset)\n","    train_loader = data.DataLoader(train_dset, **loader_args)\n","    \n","    train_losses, test_losses = train_epochs(model, train_loader, criterion, train_args=train_args, plot=False)\n","\n","    \n","\n","    if losses == []:\n","      losses = train_losses\n","    else:\n","      losses = np.concatenate((losses, np.array(train_losses)))\n","    if n_iterations % iter_plot == 0:\n","      plt.plot(losses)\n","      plt.show()   "],"metadata":{"id":"EfWlo3gsxXZu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Dagger_Lander = Lander_Net()\n","device = \"cpu\"\n","loader_args = dict(batch_size=128, shuffle=True)\n","\n","\n","train_args = dict(epochs=100, lr=0.001)\n","\n","env = gym.make('LunarLander-v2', new_step_api=True)\n","DAgger(env, expert_lander, Dagger_Lander)"],"metadata":{"id":"BRPigaZQx9jR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["anim = generate_video(env, Dagger_Lander, 500)\n","HTML(anim.to_html5_video())"],"metadata":{"id":"fpV2HGCnx_wJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Windy_Dagger_Lander = Lander_Net()\n","device = \"cpu\"\n","loader_args = dict(batch_size=128, shuffle=True)\n","train_args = dict(epochs=100, lr=0.001)\n","\n","DAgger(env, expert_windy_lander, Windy_Dagger_Lander, n_iterations = 10, n_traj=50, iter_plot=1)"],"metadata":{"id":"gtraHzW-yBo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["anim = generate_video(env, Windy_Dagger_Lander, 500)\n","HTML(anim.to_html5_video())"],"metadata":{"id":"p5WU3DUayD4r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MaxEnt IRL"],"metadata":{"id":"dYJ0586gyGEV"}},{"cell_type":"markdown","source":["It would be great if we could avoid having the expert in the loop while making sure that different initial states or stochastic dynamics have a limited impoact on the agen performance. That is the aim of Inverse RL. Instead of learning a policy given a reward, we learn a reward given expert demonstrations. The aim is to retrieve the reward function that the expert followed and then use that learned reward to train a classic RL algorithm. "],"metadata":{"id":"RDRC4NsXyKdL"}},{"cell_type":"markdown","source":["Below is the algorithm pseudocode. \n","\n","<b>Initialise</b>:<br>\n","- feature_matrix as an identity matrix of dimension number_of_states <br>\n","- expert feature expectations as a vector of size number_of_states \n","- environment to get initial state $s$<br>\n","- q_table $Q$<br>\n","- $\\theta$ and learner_feature_expectations as vectors of size number_of_states ($\\theta$ values are negative)<br>\n","- $\\gamma$ the discount factor\n","- learning_rate\n","- update_freq: frequency of reward update\n","<b>FOR</b> episode in number_episodes: <br>\n","$\\quad$ <b> IF </b> episode % update==0: <br>\n","$\\quad \\quad$ gradient = expert_feature_expectations - learner_feature_expectations/episode <br>\n","$\\quad \\quad$ $\\theta$ += learning_rate*gradient <br>\n","$\\quad \\quad$ clip $\\theta$: all values that are greater to 0 are fixed to 0 <br>\n","$\\quad$ <b> While</b> True: <br>\n","$\\quad \\quad$ Draw $a\\sim\\mathcal{U}[0,1]$ <br>\n","$\\quad \\quad$ <b>IF</b>: $a<\\epsilon$ <br>\n","$\\quad \\quad\\quad$ Draw action randomly for action space <br>\n","$\\quad \\quad$ <b> ELSE </b>: action = $\\argmax_a$ q_table[$s$] <br>\n","$\\quad \\quad$ next_state = env(action) <br>\n","$\\quad \\quad$ reward = $\\theta$.$\\text{feature_matrix[next_state]}^T$ <br>\n","$\\quad \\quad$ $Q(s,action) = reward+\\gamma \\max_aQ(next state,a)$ <br>\n","$\\quad \\quad$  learner_feature_expectations += feature_matrix(state) <br>\n","$\\quad \\quad$ <b>IF</b> done: break"],"metadata":{"id":"mEzuP_0UyNVK"}},{"cell_type":"code","source":["env = gym.make(\"MountainCar-v0\")\n","#model = PPO(MlpPolicy, env, verbose=1)\n","#model.learn(total_timesteps=250000)"],"metadata":{"id":"bHwzvVV5yHk1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model.save(PATH + \"mountain_car.zip\")"],"metadata":{"id":"nFf-4AyCyQ9g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["expert_car = PPO.load(PATH + \"mountain_car.zip\")"],"metadata":{"id":"t-y6jrfqySbn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make(\"MountainCar-v0\", new_step_api=True)\n","anim = generate_video(env, expert_car, 400)\n","HTML(anim.to_html5_video())"],"metadata":{"id":"04B1SlpwyT-Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["car_demos = generate_expert_traj(expert_car, env)"],"metadata":{"id":"r_5mm9Z7yWCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The state space of mountain car is continous. As we use Q-learning we need finite state spaces. We therefore discretize it\n","# The state space will become an array n_bins*n_bins (one dimension per state dimension)\n","n_actions = env.action_space.n\n","n_bins = 30\n","n_states = n_bins**2\n","feature_matrix = np.eye((n_states))\n","\n","#For each state, this function associates it to the corresponding state index and builds the feature vector.\n","#The feature vector is a vector of shape n_bins with a 1 at the state index\n","def feature_vector(state, env=env, bins=n_bins):\n","  try:\n","    idx = np.array(np.arange(bins**2)).reshape(bins,bins)\n","    bin_length = (env.observation_space.high - env.observation_space.low)/(bins-1)\n","    bin_id = (state - env.observation_space.low)/bin_length\n","    vector_id = idx[int(bin_id[0]), int(bin_id[1])]\n","  except:\n","    print(state, env.observation_space.high, env.observation_space.low,  bin_length, bin_id)\n","  return feature_matrix[vector_id], vector_id\n","\n","#This function classifies the state-action pairs that belong to the same trajectory\n","def process_demos(demos):\n","  demonstrations = {}\n","  demonstrations['obs'] = []\n","  demonstrations[\"actions\"] = []\n","  new_episodes_idx = np.where(demos[\"episode_starts\"]==True)[0]\n","  for i in range(new_episodes_idx.shape[0]-1):\n","    demonstrations['obs'].append(demos['obs'][new_episodes_idx[i]:new_episodes_idx[i+1]])\n","    demonstrations['actions'].append(demos['actions'][new_episodes_idx[i]:new_episodes_idx[i+1]])\n","  return demonstrations\n","\n","#Build expert_feature_expectations from demonstrations\n","def expert_feature_expectations(feature_matrix, demonstrations, n_states=n_states):\n","  n_demonstrations = len(demonstrations['obs'])\n","  feature_expectations = np.zeros((n_demonstrations,n_states))\n","  #BEGIN ANSWER\n","  #TO DO: build feature expectation for each demonstration\n","  #END ANSWER\n","  return feature_expectations"],"metadata":{"id":"P8YFwIu2yZkh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["demonstrations = process_demos(car_demos)\n","expert_expectations = expert_feature_expectations(feature_matrix, demonstrations)"],"metadata":{"id":"FWjgzemDynyD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This function calculates the gradient, updates theta abd clips it\n","def maxent_irl(expert, learner, theta, learning_rate):\n","  #BEGIN ANSWER\n","  # gradient = ...\n","  # theta += ...\n","\n","    # Clip theta\n","\n","  #END ANSWER\n","  return theta\n","\n","#Calculates the irl_reward of a given state\n","def get_reward(state, theta):\n","  #BEGIN ANSWER\n","  # irl_reward = ...\n","  #END ANSWER\n","  return irl_reward\n","\n","#Performs Q-Learning update\n","def update_q_table(q_table, state, action, reward, next_state):\n","  #BEGIN ANSWER\n","  #q_1 = ...  Q-value of state-action pair\n","  #q_2 = ...  Q-value of next state\n","  #q_table[state][action] = ... Update\n","  #END ANSWER\n","  return q_table"],"metadata":{"id":"_yd-jbrOypuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make(\"MountainCar-v0\")\n","learner_feature_expectations = np.zeros(n_states)\n","theta = - np.random.uniform(size=(n_states,))\n","q_table = np.zeros((n_states, n_actions))\n","gamma = 0.99\n","q_learning_rate = 0.03\n","theta_learning_rate = 0.05\n","eps = 0.2\n","\n","episodes, scores, mean_scores = [], [], []\n","\n","for episode in tqdm_notebook(range(30000),desc=\"Episode\", leave=True):\n","  state = env.reset()\n","  score = 0\n","\n","  if episode !=0 and episode % 5000 == 0:\n","    learner = learner_feature_expectations / episode\n","    theta = maxent_irl(expert_expectations, learner, theta, theta_learning_rate)\n","  \n","  if episode !=0 and episode % 1000 == 0:\n","    plt.plot(mean_scores)\n","    plt.show()\n","\n","  while True:\n","    _, state_id = feature_vector(state)\n","    a = np.random.uniform()\n","    if a < eps:\n","      action = env.action_space.sample()\n","    else:\n","      action = np.argmax(q_table[state_id])\n","\n","    next_state, reward, done, _ = env.step(action)\n","    irl_reward = get_reward(state, theta)\n","    _, next_state_id = feature_vector(next_state)\n","    q_table = update_q_table(q_table, state_id, action, irl_reward, next_state_id)\n","    learner_feature_expectations += feature_matrix[int(state_id)]\n","    score += reward\n","    state = next_state\n","\n","    if done:\n","      scores.append(score)\n","      episodes.append(episode)\n","      mean_scores.append(np.mean(scores))\n","      break\n"],"metadata":{"id":"I9GI3AfOzX-d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_car_video(env, q_table, n_timesteps=300):\n","  obs = env.reset()\n","  figure(figsize=(8, 6), dpi=80)\n","\n","  # use False con Xvfb \n","  _display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n","  _ = _display.start()\n","\n","  frames = []\n","  for t in range(n_timesteps):\n","    state_id = feature_vector(obs)[1]\n","    action = np.argmax(q_table[state_id])\n","    obs, rewards, done, info = env.step(action)\n","    frame = env.render(mode='rgb_array')\n","    frames.append(frame)\n","    time.sleep(.025)\n","\n","  anim = plot_animation(frames)\n","  return anim"],"metadata":{"id":"iLhVkQEKza-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["anim = generate_car_video(env, q_table)\n","HTML(anim.to_html5_video())"],"metadata":{"id":"bqdQ5zXdzdFK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We were able to solve mountain car using our learned reward. A final remark is that each time the reward is updates, the MDP has to be solved with the new reward. Solving an RL problem is usually hard enough so solving it multiple times is nearly computationally  infeasible for environment that are bigger than mountain car. \n","\n","Getting rid of that constraint is one of the main research topic on this subject. Several approaches solved it. Those interested can have a look at:\n","- [GAIL](https://arxiv.org/abs/1606.03476)\n","- [Guided Cost Learning](https://arxiv.org/abs/1603.00448)"],"metadata":{"id":"Y9LAXSSzzhxx"}},{"cell_type":"code","source":[],"metadata":{"id":"1tD1MycSzgpP"},"execution_count":null,"outputs":[]}]}